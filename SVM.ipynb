{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:23.184348200Z",
     "start_time": "2024-02-19T15:59:19.947499100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_18356\\3169084638.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to C:\\Desktop\\training\\TRAINING\\lowercased.csv, this datset has all chars lowered\n",
      "Cleaned dataset saved to C:\\Desktop\\training\\TRAINING\\no_websites.csv, this removed websites\n",
      "Cleaned dataset saved to C:\\Desktop\\training\\TRAINING\\conc.csv, this removed concs\n"
     ]
    }
   ],
   "source": [
    "original_file_path = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\training.csv'\n",
    "df = pd.read_csv(original_file_path, delimiter='\\t', encoding='utf-8')\n",
    "corrected_column = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\corrected_column.csv'\n",
    "df.to_csv(corrected_column, index=False, sep=',')\n",
    "df['Text Transcription'] = df['Text Transcription'].str.lower()\n",
    "lowercased = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\lowercased.csv'\n",
    "df.to_csv(lowercased, index=False, sep=',')\n",
    "print(f\"Cleaned dataset saved to {lowercased}, this datset has all chars lowered\")\n",
    "\n",
    "df['Text Transcription'] = df['Text Transcription'].replace(r'\\b\\w+\\.(com|org|net|zip|co)\\b', '', regex=True)\n",
    "no_websites = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\no_websites.csv'\n",
    "df.to_csv(no_websites, index=False, sep=',')\n",
    "print(f\"Cleaned dataset saved to {no_websites}, this removed websites\")\n",
    "\n",
    "df['Text Transcription'] = df['Text Transcription'].apply(lambda x: contractions.fix(x))\n",
    "conc = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\conc.csv'\n",
    "df.to_csv(conc, index=False, sep=',')\n",
    "print(f\"Cleaned dataset saved to {conc}, this removed concs\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:30.831344500Z",
     "start_time": "2024-02-19T15:59:30.225577400Z"
    }
   },
   "id": "914528a8c63c1c4f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to C:\\Desktop\\training\\TRAINING\\no_multiple_space_or_special_char.csv, removing special chars and handling multiple spaces\n"
     ]
    }
   ],
   "source": [
    "#removing special chars and handling multiple spaces\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Remove single quotes without adding a space\n",
    "    cleaned_text = re.sub(r\"'\", '', text)\n",
    "    \n",
    "    # Replace other special characters with spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', cleaned_text)\n",
    "    \n",
    "    # Replace two or more consecutive whitespace characters with a single space\n",
    "    cleaned_text = re.sub(r'\\s{2,}', ' ', cleaned_text)\n",
    "    \n",
    "    return cleaned_text.strip()  # Remove leading and trailing spaces\n",
    "\n",
    "df['Text Transcription'] = df['Text Transcription'].apply(remove_special_characters)\n",
    "no_multiple_space_or_special_char = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\no_multiple_space_or_special_char.csv'\n",
    "df.to_csv(no_multiple_space_or_special_char, index=False)\n",
    "print(f\"Cleaned dataset saved to {no_multiple_space_or_special_char}, removing special chars and handling multiple spaces\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:33.921758500Z",
     "start_time": "2024-02-19T15:59:33.746704100Z"
    }
   },
   "id": "d4ca044048f5b082"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to C:\\Desktop\\training\\TRAINING\\(final)no_num.csv, removing numbers\n"
     ]
    }
   ],
   "source": [
    "#optional (no nums)\n",
    "def remove_numbers(text):\n",
    "    # Remove numbers\n",
    "    cleaned_text = re.sub(r'\\d', '', text)\n",
    "    return cleaned_text.strip()  # Remove leading and trailing spaces\n",
    "\n",
    "df['Text Transcription'] = df['Text Transcription'].apply(remove_numbers)\n",
    "no_num = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\(final)no_num.csv'\n",
    "df.to_csv(no_num, index=False)\n",
    "print(f\"Cleaned dataset saved to {no_num}, removing numbers\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:37.802583900Z",
     "start_time": "2024-02-19T15:59:37.697575600Z"
    }
   },
   "id": "ac3880fb95761fe6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and their TF-IDF scores:\n",
      "women         187.961589\n",
      "like          144.112443\n",
      "woman         120.236758\n",
      "girlfriend    106.869148\n",
      "get            98.219847\n",
      "                 ...    \n",
      "clones          0.036755\n",
      "homed           0.036755\n",
      "dow             0.034616\n",
      "horned          0.034616\n",
      "wolverine       0.034616\n",
      "Length: 18006, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the text into words\n",
    "df['Text Transcription'] = df['Text Transcription'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Text Transcription'] = df['Text Transcription'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# Join the tokenized words back into sentences for TfidfVectorizer\n",
    "df['Text Transcription'] = df['Text Transcription'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Calculate TF-IDF scores\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text Transcription'])\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with TF-IDF scores\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Sum the TF-IDF scores across all documents (rows) for each word\n",
    "word_tfidf_sums = tfidf_df.sum()\n",
    "\n",
    "# Print the words and their TF-IDF scores\n",
    "print(\"Words and their TF-IDF scores:\")\n",
    "print(word_tfidf_sums.sort_values(ascending=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:43.982791800Z",
     "start_time": "2024-02-19T15:59:38.794508700Z"
    }
   },
   "id": "68ea3e5379ee8f71"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with the highest TF-IDF scores:\n",
      "women         187.961589\n",
      "like          144.112443\n",
      "woman         120.236758\n",
      "girlfriend    106.869148\n",
      "get            98.219847\n",
      "wife           96.974086\n",
      "kitchen        93.036649\n",
      "men            87.477241\n",
      "girl           85.636967\n",
      "girls          83.413993\n",
      "one            81.037913\n",
      "man            80.804864\n",
      "house          80.404756\n",
      "call           76.069248\n",
      "make           75.962371\n",
      "want           74.774163\n",
      "know           68.309412\n",
      "female         65.147150\n",
      "see            61.380508\n",
      "people         61.078490\n",
      "dtype: float64\n",
      "Words with the lowest TF-IDF scores:\n",
      "dow               0.034616\n",
      "horned            0.034616\n",
      "wolverine         0.034616\n",
      "clones            0.036755\n",
      "homed             0.036755\n",
      "istocrastina      0.052496\n",
      "sabers            0.052496\n",
      "amer              0.054205\n",
      "asla              0.054205\n",
      "bandhan           0.054205\n",
      "frou              0.054205\n",
      "garad             0.054205\n",
      "joen              0.054205\n",
      "kamla             0.054205\n",
      "pasin             0.054205\n",
      "petmal            0.054205\n",
      "zebiv             0.054205\n",
      "zeecinemakaml     0.054205\n",
      "zeecinemasanla    0.054205\n",
      "androgynei        0.064202\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the words with the highest TF-IDF scores\n",
    "print(\"Words with the highest TF-IDF scores:\")\n",
    "print(word_tfidf_sums.nlargest(20))\n",
    "\n",
    "# Print the words with the lowest TF-IDF scores\n",
    "print(\"Words with the lowest TF-IDF scores:\")\n",
    "print(word_tfidf_sums.nsmallest(20))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:49.531449Z",
     "start_time": "2024-02-19T15:59:49.506659Z"
    }
   },
   "id": "fcad1fd946d38080"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create a DataFrame for the words with the highest TF-IDF scores\n",
    "highest_tfidf_words = word_tfidf_sums.nlargest(20).reset_index()\n",
    "highest_tfidf_words.columns = ['Word', 'TF-IDF Score']\n",
    "\n",
    "# Create a DataFrame for the words with the lowest TF-IDF scores\n",
    "lowest_tfidf_words = word_tfidf_sums.nsmallest(20).reset_index()\n",
    "lowest_tfidf_words.columns = ['Word', 'TF-IDF Score']\n",
    "\n",
    "# Merge both DataFrames\n",
    "tfidf_words_combined = pd.concat([highest_tfidf_words, lowest_tfidf_words], axis=0)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "tfidf_words_combined.to_csv('tfidf_scores_combined.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:59:51.784486400Z",
     "start_time": "2024-02-19T15:59:51.732949400Z"
    }
   },
   "id": "91d3ba84cfe3e846"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e428f9abb8b7a1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
